#----------------------------------------------------
# Version 0.0.20
# Updated: 2/1/2026
#
#
# To Install:
#    wget -O "docker-compose.yml" https://raw.githubusercontent.com/c2theg/srvBuilds/refs/heads/master/install_ai_compose.txt && docker compose up -d
#
# To ttop and remove all:
#  docker compose down
#----------------------------------------------------
services:
  ollama:
    container_name: "ollama"
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "11434:11434"  # Exposes API for automation tasks
    volumes:
      - /usr/share/ollama/models:/root/.ollama/models
    environment:
      - OLLAMA_HOST=0.0.0.0 # Allows external API access
      - NVIDIA_VISIBLE_DEVICES=all # Makes all GPUs visible to the container

    healthcheck:     # ADD THIS: Tells Docker how to know Ollama is "Ready"
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 5s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: ["gpu"]
            count: all  # Adjust count for the number of GPUs you want to use


  open-webui:
    container_name: "ollama_openwebui"
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - open-webui-data:/app/backend/data
    depends_on:
      - ollama

volumes:
  open-webui-data:
